{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download joebeachcapital/gpa-and-iq\n",
        "! unzip /content/gpa-and-iq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eja1DkljnVHt",
        "outputId": "843b2aa7-1b36-499a-f965-2f5a615ca750"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Downloading gpa-and-iq.zip to /content\n",
            "  0% 0.00/795 [00:00<?, ?B/s]\n",
            "100% 795/795 [00:00<00:00, 2.15MB/s]\n",
            "Archive:  /content/gpa-and-iq.zip\n",
            "  inflating: gpa_iq.csv              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ATUrEyPnU-N",
        "outputId": "caa5b185-1f3a-4f83-bf39-637e910e5349"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=c77ed3f7cb7264de83f77c830412e3ce7e07b43280d425bb9a793e6bb117523b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -sSOL https://dlcdn.apache.org/kafka/3.5.0/kafka_2.13-3.5.0.tgz\n",
        "\n",
        "!tar -xzf kafka_2.13-3.5.0.tgz"
      ],
      "metadata": {
        "id": "avkLebJ1nUvW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.5.0/config/zookeeper.properties\n",
        "!./kafka_2.13-3.5.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.5.0/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0u4Ktz3ncVL",
        "outputId": "65b29f5b-156b-4103-c6a6-3525e0f78b93"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic gpa\n",
        "!./kafka_2.13-3.5.0/bin/kafka-topics.sh  --list --bootstrap-server localhost:9092"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhBCkeSznhVk",
        "outputId": "7b549010-59f2-497c-8f85-3bc68d9d57e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic gpa.\n",
            "gpa\n",
            "gpa_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "csvfile = open('gpa_iq.csv', 'r')\n",
        "jsonfile = open('gpa_iq.json', 'w')\n",
        "\n",
        "\n",
        "reader = csv.DictReader(csvfile)\n",
        "for row in reader:\n",
        "   json.dump(row, jsonfile)\n",
        "   jsonfile.write('\\n')\n",
        "\n",
        "jsonfile = open('gpa_iq.json', 'w')"
      ],
      "metadata": {
        "id": "tgEuq4S9nk07"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic gpa < gpa_iq.json\n",
        "!./kafka_2.13-3.5.0/bin/kafka-console-consumer.sh --topic gpa  --bootstrap-server localhost:9092 --from-beginning  --max-messages 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypxQPdcenpJC",
        "outputId": "020e9176-f891-4214-8e63-344f3a011fbe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"obs\": \"1\", \"gpa\": \"7.94\", \"iq\": \"111\", \"gender\": \"2\", \"concept\": \"67\"}\n",
            "{\"obs\": \"2\", \"gpa\": \"8.292\", \"iq\": \"107\", \"gender\": \"2\", \"concept\": \"43\"}\n",
            "{\"obs\": \"3\", \"gpa\": \"4.643\", \"iq\": \"100\", \"gender\": \"2\", \"concept\": \"52\"}\n",
            "{\"obs\": \"4\", \"gpa\": \"7.47\", \"iq\": \"107\", \"gender\": \"2\", \"concept\": \"66\"}\n",
            "{\"obs\": \"5\", \"gpa\": \"8.882\", \"iq\": \"114\", \"gender\": \"1\", \"concept\": \"58\"}\n",
            "{\"obs\": \"6\", \"gpa\": \"7.585\", \"iq\": \"115\", \"gender\": \"2\", \"concept\": \"51\"}\n",
            "{\"obs\": \"7\", \"gpa\": \"7.65\", \"iq\": \"111\", \"gender\": \"2\", \"concept\": \"71\"}\n",
            "{\"obs\": \"8\", \"gpa\": \"2.412\", \"iq\": \"97\", \"gender\": \"2\", \"concept\": \"51\"}\n",
            "{\"obs\": \"9\", \"gpa\": \"6\", \"iq\": \"100\", \"gender\": \"1\", \"concept\": \"49\"}\n",
            "{\"obs\": \"10\", \"gpa\": \"8.833\", \"iq\": \"112\", \"gender\": \"2\", \"concept\": \"51\"}\n",
            "{\"obs\": \"11\", \"gpa\": \"7.47\", \"iq\": \"104\", \"gender\": \"1\", \"concept\": \"35\"}\n",
            "{\"obs\": \"12\", \"gpa\": \"5.528\", \"iq\": \"89\", \"gender\": \"1\", \"concept\": \"54\"}\n",
            "{\"obs\": \"13\", \"gpa\": \"7.167\", \"iq\": \"104\", \"gender\": \"2\", \"concept\": \"54\"}\n",
            "{\"obs\": \"14\", \"gpa\": \"7.571\", \"iq\": \"102\", \"gender\": \"1\", \"concept\": \"64\"}\n",
            "{\"obs\": \"15\", \"gpa\": \"4.7\", \"iq\": \"91\", \"gender\": \"1\", \"concept\": \"56\"}\n",
            "Processed a total of 15 messages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywvI0P4alopd",
        "outputId": "5fb5336b-caf4-4b65-8f76-ca642858d93a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hdfs\n",
            "  Downloading hdfs-2.7.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from hdfs)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (2.31.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2023.7.22)\n",
            "Building wheels for collected packages: hdfs, docopt\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.2-py3-none-any.whl size=34168 sha256=e453ccdf36ad98ecb7c93fe292786a3c0fc87382f9b51e715527f1487acac0fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/39/8e/e1905de9af8ae74911cd3e53e721995cd230816f63776e5825\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=70853f953551c07dd094796634838e172b5e6b0b6f642ac87163346d052a1b88\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built hdfs docopt\n",
            "Installing collected packages: docopt, hdfs\n",
            "Successfully installed docopt-0.6.2 hdfs-2.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export SPARK_KAFKA_VERSION=0.10\n",
        "!pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.kafka:kafka-clients:3.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXuVVyXhlUbn",
        "outputId": "2f54add0-41e4-4ca2-991e-156316249fee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
            "org.apache.kafka#kafka-clients added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e49c540e-6428-4f71-9102-ba62495a73ad;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
            "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
            "\tfound commons-logging#commons-logging;1.1.3 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
            "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
            "\tfound org.apache.kafka#kafka-clients;3.3.1 in central\n",
            "\tfound com.github.luben#zstd-jni;1.5.2-1 in central\n",
            "\tfound org.lz4#lz4-java;1.8.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.1/spark-sql-kafka-0-10_2.12-3.3.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1!spark-sql-kafka-0-10_2.12.jar (41ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.3.1/kafka-clients-3.3.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.3.1!kafka-clients.jar (310ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.1/spark-token-provider-kafka-0-10_2.12-3.3.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1!spark-token-provider-kafka-0-10_2.12.jar (12ms)\n",
            "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
            "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (13ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (20ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (10ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (704ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (124ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (18ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
            "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (8ms)\n",
            "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar ...\n",
            "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.5.2-1!zstd-jni.jar (41ms)\n",
            "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
            "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (21ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.36!slf4j-api.jar (8ms)\n",
            ":: resolution report :: resolve 4372ms :: artifacts dl 1360ms\n",
            "\t:: modules in use:\n",
            "\tcom.github.luben#zstd-jni;1.5.2-1 from central in [default]\n",
            "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
            "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
            "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;3.3.1 from central in [default]\n",
            "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
            "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
            "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
            "\t:: evicted modules:\n",
            "\torg.apache.kafka#kafka-clients;2.8.1 by [org.apache.kafka#kafka-clients;3.3.1] in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.32 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   15  |   14  |   14  |   2   ||   13  |   13  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-e49c540e-6428-4f71-9102-ba62495a73ad\n",
            "\tconfs: [default]\n",
            "\t13 artifacts copied, 0 already retrieved (62765kB/133ms)\n",
            "23/09/07 11:47:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.10.12 (main, Jun 11 2023 05:26:28)\n",
            "Spark context Web UI available at http://dd0326ae0afb:4040\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1694087275293).\n",
            "SparkSession available as 'spark'.\n",
            ">>> from pyspark.sql import functions as F\n",
            ">>> from pyspark.sql.types import StructType, StringType, FloatType\n",
            ">>> import shutil\n",
            ">>> kafka_brokers = \"localhost:9092\"\n",
            ">>> def rm_checkpoint():\n",
            "...   checkpoint_location = \"checkpoints/duplicates_console_chk\"\n",
            "...   try:\n",
            "...     shutil.rmtree(checkpoint_location)\n",
            "...     print(f\"Чекпоинт в {checkpoint_location} успешно удален.\")\n",
            "...   except FileNotFoundError:\n",
            "...     print(f\"Чекпоинт в {checkpoint_location} не найден.\")\n",
            "... \n",
            ">>> raw_orders = spark.readStream.     format(\"kafka\").     option(\"kafka.bootstrap.servers\", kafka_brokers).     option(\"subscribe\", \"gpa\").     option(\"startingOffsets\", \"earliest\").     option(\"maxOffsetsPerTrigger\", \"5\").     load()\n",
            ">>> schema = StructType()     .add(\"obs\", StringType())     .add(\"gpa\", StringType())     .add(\"iq\", StringType())     .add(\"gender\", StringType())     .add(\"concept\", StringType())\n",
            ">>> extended_gpa = raw_orders     .select(F.from_json(F.col(\"value\").cast(\"String\"), schema).alias(\"value\"), \"offset\")     .select(\"value.*\", \"offset\")     .withColumn(\"receive_time\", F.current_timestamp())\n",
            ">>> extended_gpa.printSchema()\n",
            "root\n",
            " |-- obs: string (nullable = true)\n",
            " |-- gpa: string (nullable = true)\n",
            " |-- iq: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- concept: string (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- receive_time: timestamp (nullable = false)\n",
            "\n",
            ">>> def console_output(df, freq):\n",
            "...     return df.writeStream         .format(\"console\")         .trigger(processingTime='%s seconds' % freq )         .option(\"checkpointLocation\", \"checkpoints/duplicates_console_chk\")         .options(truncate=False)         .start()\n",
            "... \n",
            ">>> stream = console_output(extended_gpa , 10)\n",
            "23/09/07 11:50:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/07 11:50:49 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time           |\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|1  |7.94 |111|2     |67     |0     |2023-09-07 11:50:50.846|\n",
            "|2  |8.292|107|2     |43     |1     |2023-09-07 11:50:50.846|\n",
            "|3  |4.643|100|2     |52     |2     |2023-09-07 11:50:50.846|\n",
            "|4  |7.47 |107|2     |66     |3     |2023-09-07 11:50:50.846|\n",
            "|5  |8.882|114|1     |58     |4     |2023-09-07 11:50:50.846|\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time           |\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|6  |7.585|115|2     |51     |5     |2023-09-07 11:50:56.144|\n",
            "|7  |7.65 |111|2     |71     |6     |2023-09-07 11:50:56.144|\n",
            "|8  |2.412|97 |2     |51     |7     |2023-09-07 11:50:56.144|\n",
            "|9  |6    |100|1     |49     |8     |2023-09-07 11:50:56.144|\n",
            "|10 |8.833|112|2     |51     |9     |2023-09-07 11:50:56.144|\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time           |\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|11 |7.47 |104|1     |35     |10    |2023-09-07 11:51:00.017|\n",
            "|12 |5.528|89 |1     |54     |11    |2023-09-07 11:51:00.017|\n",
            "|13 |7.167|104|2     |54     |12    |2023-09-07 11:51:00.017|\n",
            "|14 |7.571|102|1     |64     |13    |2023-09-07 11:51:00.017|\n",
            "|15 |4.7  |91 |1     |56     |14    |2023-09-07 11:51:00.017|\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 3\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time           |\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|16 |8.167|114|1     |69     |15    |2023-09-07 11:51:10.011|\n",
            "|17 |7.822|114|1     |55     |16    |2023-09-07 11:51:10.011|\n",
            "|18 |7.598|103|1     |65     |17    |2023-09-07 11:51:10.011|\n",
            "|19 |4    |106|2     |40     |18    |2023-09-07 11:51:10.011|\n",
            "|20 |6.231|105|1     |66     |19    |2023-09-07 11:51:10.011|\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "\n",
            "stream.stop()\n",
            ">>> waterwarked_gpa = extended_gpa.withWatermark(\"receive_time\", \"30 seconds\")\n",
            ">>> waterwarked_gpa.printSchema()\n",
            "root\n",
            " |-- obs: string (nullable = true)\n",
            " |-- gpa: string (nullable = true)\n",
            " |-- iq: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- concept: string (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- receive_time: timestamp (nullable = false)\n",
            "\n",
            ">>> rm_checkpoint()\n",
            "Чекпоинт в checkpoints/duplicates_console_chk успешно удален.\n",
            ">>> stream = console_output(deduplicated_gpa , 20)\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "NameError: name 'deduplicated_gpa' is not defined\n",
            ">>> deduplicated_gpa = waterwarked_gpa.drop_duplicates([\"gender\", \"receive_time\"])\n",
            ">>> stream = console_output(deduplicated_gpa , 20)\n",
            "23/09/07 11:52:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/07 11:52:35 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time           |\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|5  |8.882|114|1     |58     |4     |2023-09-07 11:52:35.495|\n",
            "|1  |7.94 |111|2     |67     |0     |2023-09-07 11:52:35.495|\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+----------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time          |\n",
            "+---+-----+---+------+-------+------+----------------------+\n",
            "|6  |7.585|115|2     |51     |5     |2023-09-07 11:52:51.91|\n",
            "|9  |6    |100|1     |49     |8     |2023-09-07 11:52:51.91|\n",
            "+---+-----+---+------+-------+------+----------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time           |\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "|13 |7.167|104|2     |54     |12    |2023-09-07 11:53:04.683|\n",
            "|11 |7.47 |104|1     |35     |10    |2023-09-07 11:53:04.683|\n",
            "+---+-----+---+------+-------+------+-----------------------+\n",
            "\n",
            "stream.stop()\n",
            "23/09/07 11:53:22 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]] is aborting.\n",
            "23/09/07 11:53:22 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]] aborted.\n",
            "23/09/07 11:53:22 WARN Shell: Interrupted while joining on: Thread[Thread-6805,5,main]\n",
            "java.lang.InterruptedException\n",
            "\tat java.base/java.lang.Object.wait(Native Method)\n",
            "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
            "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
            "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
            "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
            "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
            "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
            "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
            "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
            "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:212)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1113)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1102)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1073)\n",
            "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:133)\n",
            "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:747)\n",
            "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
            "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)\n",
            "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
            "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
            "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:377)\n",
            "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
            "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
            "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
            "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
            "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
            "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.$anonfun$doExecute$24(statefulOperators.scala:879)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
            "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
            "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.timeTakenMs(statefulOperators.scala:816)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.$anonfun$doExecute$22(statefulOperators.scala:879)\n",
            "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
            "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:464)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "23/09/07 11:53:22 WARN Shell: Interrupted while joining on: Thread[Thread-6806,5,main]\n",
            "java.lang.InterruptedException\n",
            "\tat java.base/java.lang.Object.wait(Native Method)\n",
            "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
            "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
            "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1042)\n",
            "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n",
            "\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\n",
            "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\n",
            "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\n",
            "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\n",
            "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\n",
            "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\n",
            "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\n",
            "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
            "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
            "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n",
            "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n",
            "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n",
            "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
            "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n",
            "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:366)\n",
            "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n",
            "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n",
            "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:372)\n",
            "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:110)\n",
            "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:110)\n",
            "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:111)\n",
            "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:111)\n",
            "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.$anonfun$doExecute$24(statefulOperators.scala:879)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:640)\n",
            "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:143)\n",
            "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:143)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.timeTakenMs(statefulOperators.scala:816)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.$anonfun$doExecute$22(statefulOperators.scala:879)\n",
            "\tat org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
            "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:464)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "23/09/07 11:53:22 ERROR Utils: Aborting task\n",
            "org.apache.spark.SparkException: Commit denied for partition 33 (task 641, attempt 0, stage 11.0).\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:929)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:485)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "23/09/07 11:53:22 ERROR DataWritingSparkTask: Aborting commit for partition 33 (task 641, attempt 0, stage 11.0)\n",
            "23/09/07 11:53:22 ERROR DataWritingSparkTask: Aborted commit for partition 33 (task 641, attempt 0, stage 11.0)\n",
            "23/09/07 11:53:22 ERROR Utils: Aborting task\n",
            "org.apache.spark.SparkException: Commit denied for partition 34 (task 642, attempt 0, stage 11.0).\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.commitDeniedError(QueryExecutionErrors.scala:929)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:485)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n",
            "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "23/09/07 11:53:22 ERROR DataWritingSparkTask: Aborting commit for partition 34 (task 642, attempt 0, stage 11.0)\n",
            "23/09/07 11:53:22 ERROR DataWritingSparkTask: Aborted commit for partition 34 (task 642, attempt 0, stage 11.0)\n",
            "23/09/07 11:53:22 WARN TaskSetManager: Lost task 33.0 in stage 11.0 (TID 641) (dd0326ae0afb executor driver): TaskKilled (Stage cancelled)\n",
            "23/09/07 11:53:22 WARN TaskSetManager: Lost task 34.0 in stage 11.0 (TID 642) (dd0326ae0afb executor driver): TaskKilled (Stage cancelled)\n",
            "rm_checkpoint()\n",
            "Чекпоинт в checkpoints/duplicates_console_chk успешно удален.\n",
            ">>> windowed_gpa = extended_gpa.withColumn(\"window_time\", F.window(F.col(\"receive_time\"), \"2 minutes\"))\n",
            ">>> windowed_gpa.printSchema()\n",
            "root\n",
            " |-- obs: string (nullable = true)\n",
            " |-- gpa: string (nullable = true)\n",
            " |-- iq: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- concept: string (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- receive_time: timestamp (nullable = false)\n",
            " |-- window_time: struct (nullable = false)\n",
            " |    |-- start: timestamp (nullable = true)\n",
            " |    |-- end: timestamp (nullable = true)\n",
            "\n",
            ">>> waterwarked_windowed_gpa = windowed_gpa.withWatermark(\"window_time\", \"2 minutes\")\n",
            ">>> deduplicated_windowed_gpa = waterwarked_windowed_gpa     .drop_duplicates([\"gender\", \"window_time\"])\n",
            ">>> stream = console_output(deduplicated_windowed_gpa , 20)\n",
            "23/09/07 11:54:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/07 11:54:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+---------------------+------------------------------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time         |window_time                               |\n",
            "+---+-----+---+------+-------+------+---------------------+------------------------------------------+\n",
            "|5  |8.882|114|1     |58     |4     |2023-09-07 11:54:50.2|{2023-09-07 11:54:00, 2023-09-07 11:56:00}|\n",
            "|1  |7.94 |111|2     |67     |0     |2023-09-07 11:54:50.2|{2023-09-07 11:54:00, 2023-09-07 11:56:00}|\n",
            "+---+-----+---+------+-------+------+---------------------+------------------------------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+---+---+---+------+-------+------+------------+-----------+\n",
            "|obs|gpa|iq |gender|concept|offset|receive_time|window_time|\n",
            "+---+---+---+------+-------+------+------------+-----------+\n",
            "+---+---+---+------+-------+------+------------+-----------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+---+---+---+------+-------+------+------------+-----------+\n",
            "|obs|gpa|iq |gender|concept|offset|receive_time|window_time|\n",
            "+---+---+---+------+-------+------+------------+-----------+\n",
            "+---+---+---+------+-------+------+------------+-----------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 3\n",
            "-------------------------------------------\n",
            "+---+---+---+------+-------+------+------------+-----------+\n",
            "|obs|gpa|iq |gender|concept|offset|receive_time|window_time|\n",
            "+---+---+---+------+-------+------+------------+-----------+\n",
            "+---+---+---+------+-------+------+------------+-----------+\n",
            "\n",
            "stream.stop()\n",
            ">>> rm_checkpoint()\n",
            "Чекпоинт в checkpoints/duplicates_console_chk успешно удален.\n",
            ">>> sliding_gpa = extended_gpa.withColumn(\"sliding_time\", F.window(F.col(\"receive_time\"), \"1 minute\", \"30 seconds\"))\n",
            ">>> waterwarked_sliding_gpa = sliding_gpa.withWatermark(\"sliding_time\", \"2 minutes\")\n",
            ">>> deduplicated_sliding_gpa = waterwarked_sliding_gpa.drop_duplicates([\"gender\", \"sliding_time\"])\n",
            ">>> deduplicated_sliding_gpa.printSchema()\n",
            "root\n",
            " |-- obs: string (nullable = true)\n",
            " |-- gpa: string (nullable = true)\n",
            " |-- iq: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- concept: string (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- receive_time: timestamp (nullable = false)\n",
            " |-- sliding_time: struct (nullable = true)\n",
            " |    |-- start: timestamp (nullable = true)\n",
            " |    |-- end: timestamp (nullable = true)\n",
            "\n",
            ">>> stream = console_output(deduplicated_sliding_gpa , 20)\n",
            "23/09/07 11:57:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/07 11:57:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+-----------------------+------------------------------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time           |sliding_time                              |\n",
            "+---+-----+---+------+-------+------+-----------------------+------------------------------------------+\n",
            "|1  |7.94 |111|2     |67     |0     |2023-09-07 11:57:10.256|{2023-09-07 11:57:00, 2023-09-07 11:58:00}|\n",
            "|5  |8.882|114|1     |58     |4     |2023-09-07 11:57:10.256|{2023-09-07 11:56:30, 2023-09-07 11:57:30}|\n",
            "|5  |8.882|114|1     |58     |4     |2023-09-07 11:57:10.256|{2023-09-07 11:57:00, 2023-09-07 11:58:00}|\n",
            "|1  |7.94 |111|2     |67     |0     |2023-09-07 11:57:10.256|{2023-09-07 11:56:30, 2023-09-07 11:57:30}|\n",
            "+---+-----+---+------+-------+------+-----------------------+------------------------------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+---+---+---+------+-------+------+------------+------------+\n",
            "|obs|gpa|iq |gender|concept|offset|receive_time|sliding_time|\n",
            "+---+---+---+------+-------+------+------------+------------+\n",
            "+---+---+---+------+-------+------+------------+------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+---+-----+---+------+-------+------+-----------------------+------------------------------------------+\n",
            "|obs|gpa  |iq |gender|concept|offset|receive_time           |sliding_time                              |\n",
            "+---+-----+---+------+-------+------+-----------------------+------------------------------------------+\n",
            "|11 |7.47 |104|1     |35     |10    |2023-09-07 11:57:40.013|{2023-09-07 11:57:30, 2023-09-07 11:58:30}|\n",
            "|13 |7.167|104|2     |54     |12    |2023-09-07 11:57:40.013|{2023-09-07 11:57:30, 2023-09-07 11:58:30}|\n",
            "+---+-----+---+------+-------+------+-----------------------+------------------------------------------+\n",
            "\n",
            "stream.stop()\n",
            ">>> def console_output(df, freq, out_mode):\n",
            "...     return df.writeStream.format(\"console\")         .trigger(processingTime='%s seconds' % freq )         .options(truncate=False)         .option(\"checkpointLocation\", \"checkpoints/watermark_console_chk2\")         .outputMode(out_mode)         .start()\n",
            "... \n",
            ">>> def rm_checkpoint():\n",
            "...   checkpoint_location = \"checkpoints/watermark_console_chk2\"\n",
            "...   try:\n",
            "...     shutil.rmtree(checkpoint_location)\n",
            "...     print(f\"Чекпоинт в {checkpoint_location} успешно удален.\")\n",
            "...   except FileNotFoundError:\n",
            "...     print(f\"Чекпоинт в {checkpoint_location} не найден.\")\n",
            "... \n",
            ">>> count_gpa = waterwarked_windowed_gpa.groupBy(\"gender\").count()\n",
            ">>> rm_checkpoint()\n",
            "Чекпоинт в checkpoints/watermark_console_chk2 не найден.\n",
            ">>> stream = console_output(count_gpa , 20, \"update\")\n",
            "23/09/07 12:00:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/07 12:00:55 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|1     |1    |\n",
            "|2     |4    |\n",
            "+------+-----+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|1     |2    |\n",
            "|2     |8    |\n",
            "+------+-----+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|1     |6    |\n",
            "|2     |9    |\n",
            "+------+-----+\n",
            "\n",
            "stream.stop()\n",
            ">>> rm_checkpoint()\n",
            "Чекпоинт в checkpoints/watermark_console_chk2 успешно удален.\n",
            ">>> stream = console_output(count_gpa , 20, \"complete\")\n",
            "23/09/07 12:02:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/07 12:02:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|1     |1    |\n",
            "|2     |4    |\n",
            "+------+-----+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|1     |2    |\n",
            "|2     |8    |\n",
            "+------+-----+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|1     |6    |\n",
            "|2     |9    |\n",
            "+------+-----+\n",
            "\n",
            "stream.stop()\n",
            ">>> rm_checkpoint()\n",
            "Чекпоинт в checkpoints/watermark_console_chk2 успешно удален.\n",
            ">>> stream = console_output(count_gpa , 20, \"append\")\n",
            "23/09/07 12:07:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "  File \"<stdin>\", line 2, in console_output\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/readwriter.py\", line 1385, in start\n",
            "    return self._sq(self._jwrite.start())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
            "    raise converted from None\n",
            "pyspark.errors.exceptions.captured.AnalysisException: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n",
            "Aggregate [gender#27], [gender#27, count(1) AS count#763L]\n",
            "+- EventTimeWatermark window_time#367: struct<start: timestamp, end: timestamp>, 2 minutes\n",
            "   +- Project [obs#24, gpa#25, iq#26, gender#27, concept#28, offset#11L, receive_time#35, window#368 AS window_time#367]\n",
            "      +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(receive_time#35, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(receive_time#35, TimestampType, LongType) - 0) % 120000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(receive_time#35, TimestampType, LongType) - 0) % 120000000) + 120000000) ELSE ((precisetimestampconversion(receive_time#35, TimestampType, LongType) - 0) % 120000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(receive_time#35, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(receive_time#35, TimestampType, LongType) - 0) % 120000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(receive_time#35, TimestampType, LongType) - 0) % 120000000) + 120000000) ELSE ((precisetimestampconversion(receive_time#35, TimestampType, LongType) - 0) % 120000000) END) - 0) + 120000000), LongType, TimestampType))) AS window#368, obs#24, gpa#25, iq#26, gender#27, concept#28, offset#11L, receive_time#35]\n",
            "         +- Filter isnotnull(receive_time#35)\n",
            "            +- Project [obs#24, gpa#25, iq#26, gender#27, concept#28, offset#11L, current_timestamp() AS receive_time#35]\n",
            "               +- Project [value#21.obs AS obs#24, value#21.gpa AS gpa#25, value#21.iq AS iq#26, value#21.gender AS gender#27, value#21.concept AS concept#28, offset#11L]\n",
            "                  +- Project [from_json(StructField(obs,StringType,true), StructField(gpa,StringType,true), StructField(iq,StringType,true), StructField(gender,StringType,true), StructField(concept,StringType,true), cast(value#8 as string), Some(Etc/UTC)) AS value#21, offset#11L]\n",
            "                     +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1386cc9a, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5e267e8b, [startingOffsets=earliest, kafka.bootstrap.servers=localhost:9092, subscribe=gpa, maxOffsetsPerTrigger=5], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@74f299cd,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> gpa, startingOffsets -> earliest, maxOffsetsPerTrigger -> 5),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
            "\n",
            ">>> stream.stop()\n",
            ">>> rm_checkpoint()\n",
            "Чекпоинт в checkpoints/watermark_console_chk2 успешно удален.\n",
            ">>> sliding_gpa = waterwarked_sliding_gpa.groupBy(\"sliding_time\").count()\n",
            ">>> stream = console_output(sliding_gpa , 20, \"update\")\n",
            "23/09/07 12:08:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/07 12:08:18 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+------------------------------------------+-----+\n",
            "|sliding_time                              |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2023-09-07 12:08:00, 2023-09-07 12:09:00}|5    |\n",
            "|{2023-09-07 12:07:30, 2023-09-07 12:08:30}|5    |\n",
            "+------------------------------------------+-----+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+------------------------------------------+-----+\n",
            "|sliding_time                              |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2023-09-07 12:08:00, 2023-09-07 12:09:00}|10   |\n",
            "|{2023-09-07 12:07:30, 2023-09-07 12:08:30}|10   |\n",
            "+------------------------------------------+-----+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+------------------------------------------+-----+\n",
            "|sliding_time                              |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2023-09-07 12:08:00, 2023-09-07 12:09:00}|15   |\n",
            "|{2023-09-07 12:08:30, 2023-09-07 12:09:30}|5    |\n",
            "+------------------------------------------+-----+\n",
            "\n",
            "stream.stop()\n",
            ">>> rm_checkpoint()\n",
            "Чекпоинт в checkpoints/watermark_console_chk2 успешно удален.\n",
            ">>> static_df_schema = StructType()     .add(\"gender\", StringType())     .add(\"description\", StringType())\n",
            ">>> static_df_data = (     (\"1\", \"male\"),     (\"2\", \"female\") )\n",
            ">>> static_df = spark.createDataFrame(static_df_data, static_df_schema)\n",
            ">>> static_joined = waterwarked_gpa.join(static_df, \"gender\", \"left\")\n",
            ">>> static_joined.isStreaming\n",
            "True\n",
            ">>> static_joined.printSchema()\n",
            "root\n",
            " |-- gender: string (nullable = true)\n",
            " |-- obs: string (nullable = true)\n",
            " |-- gpa: string (nullable = true)\n",
            " |-- iq: string (nullable = true)\n",
            " |-- concept: string (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- receive_time: timestamp (nullable = false)\n",
            " |-- description: string (nullable = true)\n",
            "\n",
            ">>> stream = console_output(static_joined , 20, \"update\")\n",
            "23/09/07 12:11:07 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/07 12:11:07 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+------+---+-----+---+-------+------+----------------------+-----------+\n",
            "|gender|obs|gpa  |iq |concept|offset|receive_time          |description|\n",
            "+------+---+-----+---+-------+------+----------------------+-----------+\n",
            "|1     |5  |8.882|114|58     |4     |2023-09-07 12:11:07.47|male       |\n",
            "|2     |1  |7.94 |111|67     |0     |2023-09-07 12:11:07.47|female     |\n",
            "|2     |2  |8.292|107|43     |1     |2023-09-07 12:11:07.47|female     |\n",
            "|2     |3  |4.643|100|52     |2     |2023-09-07 12:11:07.47|female     |\n",
            "|2     |4  |7.47 |107|66     |3     |2023-09-07 12:11:07.47|female     |\n",
            "+------+---+-----+---+-------+------+----------------------+-----------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+------+---+-----+---+-------+------+----------------------+-----------+\n",
            "|gender|obs|gpa  |iq |concept|offset|receive_time          |description|\n",
            "+------+---+-----+---+-------+------+----------------------+-----------+\n",
            "|1     |9  |6    |100|49     |8     |2023-09-07 12:11:20.01|male       |\n",
            "|2     |6  |7.585|115|51     |5     |2023-09-07 12:11:20.01|female     |\n",
            "|2     |7  |7.65 |111|71     |6     |2023-09-07 12:11:20.01|female     |\n",
            "|2     |8  |2.412|97 |51     |7     |2023-09-07 12:11:20.01|female     |\n",
            "|2     |10 |8.833|112|51     |9     |2023-09-07 12:11:20.01|female     |\n",
            "+------+---+-----+---+-------+------+----------------------+-----------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+------+---+-----+---+-------+------+-----------------------+-----------+\n",
            "|gender|obs|gpa  |iq |concept|offset|receive_time           |description|\n",
            "+------+---+-----+---+-------+------+-----------------------+-----------+\n",
            "|1     |11 |7.47 |104|35     |10    |2023-09-07 12:11:40.008|male       |\n",
            "|1     |12 |5.528|89 |54     |11    |2023-09-07 12:11:40.008|male       |\n",
            "|1     |14 |7.571|102|64     |13    |2023-09-07 12:11:40.008|male       |\n",
            "|1     |15 |4.7  |91 |56     |14    |2023-09-07 12:11:40.008|male       |\n",
            "|2     |13 |7.167|104|54     |12    |2023-09-07 12:11:40.008|female     |\n",
            "+------+---+-----+---+-------+------+-----------------------+-----------+\n",
            "\n",
            "stream.stop()\n",
            ">>> rm_checkpoint()\n",
            "Чекпоинт в checkpoints/watermark_console_chk2 успешно удален.\n",
            ">>> exit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType, StringType, FloatType\n",
        "import shutil\n",
        "\n",
        "kafka_brokers = \"localhost:9092\"\n",
        "\n",
        "def rm_checkpoint():\n",
        "  checkpoint_location = \"checkpoints/duplicates_console_chk\"\n",
        "  try:\n",
        "    shutil.rmtree(checkpoint_location)\n",
        "    print(f\"Чекпоинт в {checkpoint_location} успешно удален.\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Чекпоинт в {checkpoint_location} не найден.\")\n",
        "\n",
        "raw_orders = spark.readStream.\\\n",
        "    format(\"kafka\").\\\n",
        "    option(\"kafka.bootstrap.servers\", kafka_brokers).\\\n",
        "    option(\"subscribe\", \"gpa\").\\\n",
        "    option(\"startingOffsets\", \"earliest\").\\\n",
        "    option(\"maxOffsetsPerTrigger\", \"5\").\\\n",
        "    load()\n",
        "\n",
        "schema = StructType()\\\n",
        "    .add(\"obs\", StringType())\\\n",
        "    .add(\"gpa\", StringType())\\\n",
        "    .add(\"iq\", StringType())\\\n",
        "    .add(\"gender\", StringType())\\\n",
        "    .add(\"concept\", StringType())\n",
        "\n",
        "extended_gpa = raw_orders\\\n",
        "    .select(F.from_json(F.col(\"value\").cast(\"String\"), schema).alias(\"value\"), \"offset\")\\\n",
        "    .select(\"value.*\", \"offset\")\\\n",
        "    .withColumn(\"receive_time\", F.current_timestamp())\n",
        "\n",
        "extended_gpa.printSchema()\n",
        "\n",
        "def console_output(df, freq):\n",
        "    return df.writeStream\\\n",
        "        .format(\"console\")\\\n",
        "        .trigger(processingTime='%s seconds' % freq )\\\n",
        "        .option(\"checkpointLocation\", \"checkpoints/duplicates_console_chk\")\\\n",
        "        .options(truncate=False)\\\n",
        "        .start()\n",
        "\n",
        "stream = console_output(extended_gpa , 10)\n",
        "stream.stop()\n"
      ],
      "metadata": {
        "id": "NplE0ehZfBoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waterwarked_gpa = extended_gpa.withWatermark(\"receive_time\", \"30 seconds\")\n",
        "waterwarked_gpa.printSchema()\n",
        "\n",
        "deduplicated_gpa = waterwarked_gpa.drop_duplicates([\"gender\", \"receive_time\"])\n",
        "\n",
        "#Чтобы всё заработало, нужно предварительно очистить чекпойнт (команда `hdfs dfs -rm -r \"checkpoints/duplicates_console_chk\"`).\n",
        "rm_checkpoint()\n",
        "\n",
        "stream = console_output(deduplicated_gpa , 20)\n",
        "\n",
        "#В каждом микробатче только записи с уникальным значением по колонке `species`. По значению в `offset` видно что не все сообщения попадают в вывод.\n",
        "\n",
        "stream.stop()"
      ],
      "metadata": {
        "id": "PgNMhgmyfs9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Создаём временное окно. В структуру датафрейма добавился новый столбец.\n",
        "\n",
        "rm_checkpoint()\n",
        "\n",
        "windowed_gpa = extended_gpa.withColumn(\"window_time\", F.window(F.col(\"receive_time\"), \"2 minutes\"))\n",
        "windowed_gpa.printSchema()\n",
        "\n",
        "#Устанавливаем вотермарку для очистки чекпоинта и удаляем дубли в каждом окне.\n",
        "\n",
        "waterwarked_windowed_gpa = windowed_gpa.withWatermark(\"window_time\", \"2 minutes\")\n",
        "deduplicated_windowed_gpa = waterwarked_windowed_gpa\\\n",
        "    .drop_duplicates([\"gender\", \"window_time\"])\n",
        "\n",
        "#Проверяем как удаляются дубли из каждого окна.\n",
        "\n",
        "stream = console_output(deduplicated_windowed_gpa , 20)\n",
        "\n",
        "#Видим что в рамках одного окна дубликатов нет.\n",
        "\n",
        "stream.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "heE4BkZ6gv4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SLIDING WINDOW\n",
        "\n",
        "rm_checkpoint()\n",
        "\n",
        "sliding_gpa = extended_gpa.withColumn(\"sliding_time\", F.window(F.col(\"receive_time\"), \"1 minute\", \"30 seconds\"))\n",
        "waterwarked_sliding_gpa = sliding_gpa.withWatermark(\"sliding_time\", \"2 minutes\")\n",
        "deduplicated_sliding_gpa = waterwarked_sliding_gpa.drop_duplicates([\"gender\", \"sliding_time\"])\n",
        "deduplicated_sliding_gpa.printSchema()\n",
        "\n",
        "stream = console_output(deduplicated_sliding_gpa , 20)\n",
        "\n",
        "stream.stop()\n"
      ],
      "metadata": {
        "id": "MeWpkIG_hRQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Переопределяем метод `console_output` так, чтобы можно было задавать режим вывода результата работы аггрегационных функций.\n",
        "\n",
        "def console_output(df, freq, out_mode):\n",
        "    return df.writeStream.format(\"console\")\n",
        "        .trigger(processingTime='%s seconds' % freq )\n",
        "        .options(truncate=False)\n",
        "        .option(\"checkpointLocation\", \"checkpoints/watermark_console_chk2\")\n",
        "        .outputMode(out_mode)\n",
        "        .start()\n",
        "\n",
        "def rm_checkpoint():\n",
        "  checkpoint_location = \"checkpoints/watermark_console_chk2\"\n",
        "  try:\n",
        "    shutil.rmtree(checkpoint_location)\n",
        "    print(f\"Чекпоинт в {checkpoint_location} успешно удален.\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Чекпоинт в {checkpoint_location} не найден.\")\n",
        "\n",
        "#Используем ранее созданный датафрейм с вотермаркой на 2 минуты. Группируем данные по нескользящему окну.\n",
        "\n",
        "count_gpa = waterwarked_windowed_gpa.groupBy(\"gender\").count()\n",
        "\n",
        "\n",
        "#Перед каждым запуском очищаем чекпойнт\n",
        "rm_checkpoint()\n",
        "\n",
        "stream = console_output(count_gpa , 20, \"update\")\n",
        "stream.stop()\n",
        "rm_checkpoint()\n",
        "\n",
        "#В режиме `.outputMode(\"update\")` в консоль пишутся только обновляющиеся записи. Считатся `count` по каждому окну и выводятся записи только о тех окнах, в которых значение поменялось.\n",
        "\n",
        "stream = console_output(count_gpa , 20, \"complete\")\n",
        "stream.stop()\n",
        "rm_checkpoint()\n",
        "\n",
        "#В режиме `.outputMode(\"complete\")` в консоль пишутся все  записи. Считается `count` по каждому окну и выводятся результаты подсчёта во всех окнах.\n",
        "\n",
        "stream = console_output(count_gpa , 20, \"append\")\n",
        "stream.stop()\n",
        "rm_checkpoint()\n",
        "\n",
        "#Здесь режим не поддерживается для данной аггрегирующей функции.\n",
        "\n",
        "#Наблюдаем за суммами в плавающем окне.\n",
        "\n",
        "sliding_gpa = waterwarked_sliding_gpa.groupBy(\"sliding_time\").count()\n",
        "stream = console_output(sliding_gpa , 20, \"update\")\n",
        "stream.stop()\n",
        "rm_checkpoint()\n",
        "\n",
        "#Наблюдаем только обновляющиеся записи в каждом плавающем окне.\n"
      ],
      "metadata": {
        "id": "Sg29Bh9XiR4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Создадим статический датафрейм, который будет расширять исходный датасет.\n",
        "\n",
        "static_df_schema = StructType()\n",
        "    .add(\"gender\", StringType())\n",
        "    .add(\"description\", StringType())\n",
        "\n",
        "static_df_data = (\n",
        "    (\"1\", \"male\"),\n",
        "    (\"2\", \"female\")\n",
        ")\n",
        "\n",
        "static_df = spark.createDataFrame(static_df_data, static_df_schema)\n",
        "\n",
        "static_joined = waterwarked_gpa.join(static_df, \"gender\", \"left\")\n",
        "static_joined.isStreaming\n",
        "\n",
        "#После джойна стрима со статикой получаем стрим.\n",
        "\n",
        "static_joined.printSchema()\n",
        "\n",
        "stream = console_output(static_joined , 20, \"update\")\n",
        "stream.stop()\n",
        "rm_checkpoint()"
      ],
      "metadata": {
        "id": "Y33hAt8Jjj4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}