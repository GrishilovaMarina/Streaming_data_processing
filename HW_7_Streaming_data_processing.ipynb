{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abUl0CpaPVrH"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download joebeachcapital/gpa-and-iq\n",
        "! unzip /content/gpa-and-iq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -sSOL https://dlcdn.apache.org/kafka/3.5.0/kafka_2.13-3.5.0.tgz\n",
        "!tar -xzf kafka_2.13-3.5.0.tgz"
      ],
      "metadata": {
        "id": "_dfYdw86MQul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.5.0/config/zookeeper.properties\n",
        "!./kafka_2.13-3.5.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.5.0/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jYTPdkSMVd0",
        "outputId": "dc41315d-ea9f-4076-c83b-273704281ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "csvfile = open('gpa_iq.csv', 'r')\n",
        "jsonfile = open('gpa_iq.json', 'w')\n",
        "\n",
        "\n",
        "reader = csv.DictReader(csvfile)\n",
        "for row in reader:\n",
        "   json.dump(row, jsonfile)\n",
        "   jsonfile.write('\\n')\n",
        "\n",
        "jsonfile = open('gpa_iq.json', 'w')"
      ],
      "metadata": {
        "id": "fidkIM3VOXdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic gpa < gpa_iq.json\n",
        "!./kafka_2.13-3.5.0/bin/kafka-console-consumer.sh --topic gpa  --bootstrap-server localhost:9092 --from-beginning  --max-messages 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az1xdBQ8MxXB",
        "outputId": "7a3a7baf-1f38-4c5f-f288-be67e8ad945e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-09-14 14:28:14,280] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 1 : {gpa=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)\n",
            "[2023-09-14 14:28:14,399] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 4 : {gpa=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)\n",
            "[2023-09-14 14:28:14,508] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 5 : {gpa=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)\n",
            "[2023-09-14 14:28:14,619] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 6 : {gpa=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)\n",
            "[2023-09-14 14:28:14,730] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 7 : {gpa=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)\n",
            "{\"obs\": \"1\", \"gpa\": \"7.94\", \"iq\": \"111\", \"gender\": \"2\", \"concept\": \"67\"}\n",
            "{\"obs\": \"2\", \"gpa\": \"8.292\", \"iq\": \"107\", \"gender\": \"2\", \"concept\": \"43\"}\n",
            "{\"obs\": \"3\", \"gpa\": \"4.643\", \"iq\": \"100\", \"gender\": \"2\", \"concept\": \"52\"}\n",
            "{\"obs\": \"4\", \"gpa\": \"7.47\", \"iq\": \"107\", \"gender\": \"2\", \"concept\": \"66\"}\n",
            "{\"obs\": \"5\", \"gpa\": \"8.882\", \"iq\": \"114\", \"gender\": \"1\", \"concept\": \"58\"}\n",
            "Processed a total of 5 messages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "ZyXMxQlde9Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Подготовим обученную модель\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "metadata": {
        "id": "sy3mLLTE5m0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName('hw_7') \\\n",
        "    .config('spark.executor.instances', 4) \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "QbYGc2Gb5vY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('gpa_iq.csv', header=True, inferSchema=True).drop('_c0')\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7_lzzaA5zzi",
        "outputId": "dc8b7220-3230-420e-ea63-e68175798d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- obs: integer (nullable = true)\n",
            " |-- gpa: double (nullable = true)\n",
            " |-- iq: integer (nullable = true)\n",
            " |-- gender: integer (nullable = true)\n",
            " |-- concept: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desc = df.describe()\n",
        "for col in desc.columns:\n",
        "    if col not in ['summary', 'gender']:\n",
        "        desc = desc.withColumn(col, F.round(col, 2))\n",
        "desc.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIn1TQck6BhJ",
        "outputId": "13f3f3f5-547e-4b22-f349-09ed675f86a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+------+------------------+-------+\n",
            "|summary|  obs|  gpa|    iq|            gender|concept|\n",
            "+-------+-----+-----+------+------------------+-------+\n",
            "|  count| 78.0| 78.0|  78.0|                78|   78.0|\n",
            "|   mean|42.97| 7.45|108.92|1.6025641025641026|  56.96|\n",
            "| stddev|25.89|  2.1| 13.17| 0.492535016613786|  12.41|\n",
            "|    min|  1.0| 0.53|  72.0|                 1|   20.0|\n",
            "|    max| 89.0|10.76| 136.0|                 2|   80.0|\n",
            "+-------+-----+-----+------+------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numericCols = df.drop('gender').columns\n",
        "numeric_data = df.select(numericCols).toPandas()"
      ],
      "metadata": {
        "id": "HdW5h4d86Nvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('gender').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEgFi8UC6Tf4",
        "outputId": "cf86850a-5592-4dd1-8f4b-9e0a832cc634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     1|   31|\n",
            "|     2|   47|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_stringIdx = StringIndexer(inputCol='gender', outputCol='label')\n",
        "assembler = VectorAssembler(inputCols=numericCols, outputCol='features')\n",
        "\n",
        "train, test = df.randomSplit([0.7, 0.3], seed=42)"
      ],
      "metadata": {
        "id": "HTb_dj937PQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.groupby('gender').count().show(), test.groupby('gender').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq0WgmNI7Z1v",
        "outputId": "bce3066d-8852-4dc2-c601-076b3edaeff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     1|   17|\n",
            "|     2|   28|\n",
            "+------+-----+\n",
            "\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     1|   14|\n",
            "|     2|   19|\n",
            "+------+-----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(maxIter=10)"
      ],
      "metadata": {
        "id": "V2k9oByS7pF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[label_stringIdx, assembler, lr])\n",
        "\n",
        "# обучаемся на трейне:\n",
        "lrModel = pipeline.fit(train)\n",
        "# предсказания\n",
        "train_predictions = lrModel.transform(train)\n",
        "test_predictions = lrModel.transform(test)"
      ],
      "metadata": {
        "id": "CsM4t-Li7thX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#посмотрим на метрику\n",
        "evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
        "\n",
        "train_auc_roc = evaluator.evaluate(train_predictions)\n",
        "test_auc_roc = evaluator.evaluate(test_predictions)\n",
        "\n",
        "print(f'Train AUC-ROC: {train_auc_roc}\\nTest AUC-ROC:{test_auc_roc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yixxOcPc70t-",
        "outputId": "159478a0-f89a-45e6-8650-ed63f23709eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train AUC-ROC: 0.796218487394958\n",
            "Test AUC-ROC:0.650375939849624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#сохраним модель\n",
        "lrModel.write().overwrite().save('gpa_lr_Model')"
      ],
      "metadata": {
        "id": "0Jft_3p578F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jre"
      ],
      "metadata": {
        "id": "u3_70SlnPO4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/cassandra/4.0.11/apache-cassandra-4.0.11-bin.tar.gz\n",
        "!tar -xzvf apache-cassandra-4.0.11-bin.tar.gz"
      ],
      "metadata": {
        "id": "chQkTXmnPTWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apache-cassandra-4.0.11/bin/cassandra -R"
      ],
      "metadata": {
        "id": "G9RGkzmM___4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cassandra-driver"
      ],
      "metadata": {
        "id": "W1xr-wIgNGT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsY9xq1T_Voj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462476c2-0398-4310-b009-915c405483e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
            "org.apache.kafka#kafka-clients added as a dependency\n",
            "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-af4bdf18-6e56-4d59-931f-59606815db84;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
            "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
            "\tfound commons-logging#commons-logging;1.1.3 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
            "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
            "\tfound org.apache.kafka#kafka-clients;3.3.1 in central\n",
            "\tfound com.github.luben#zstd-jni;1.5.2-1 in central\n",
            "\tfound org.lz4#lz4-java;1.8.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
            "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
            "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
            "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
            "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
            "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
            "\tfound com.typesafe#config;1.4.1 in central\n",
            "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
            "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
            "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
            "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
            "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
            "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
            "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
            "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
            "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
            "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.1/spark-sql-kafka-0-10_2.12-3.3.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1!spark-sql-kafka-0-10_2.12.jar (92ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.3.1/kafka-clients-3.3.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.3.1!kafka-clients.jar (599ms)\n",
            "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
            "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (334ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.1/spark-token-provider-kafka-0-10_2.12-3.3.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1!spark-token-provider-kafka-0-10_2.12.jar (60ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (95ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (48ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (2369ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (526ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (46ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
            "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (31ms)\n",
            "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar ...\n",
            "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.5.2-1!zstd-jni.jar (137ms)\n",
            "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
            "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (58ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.36/slf4j-api-1.7.36.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.36!slf4j-api.jar (43ms)\n",
            "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
            "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (60ms)\n",
            "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
            "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (136ms)\n",
            "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
            "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (40ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (33ms)\n",
            "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
            "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (32ms)\n",
            "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
            "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (77ms)\n",
            "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
            "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (49ms)\n",
            "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
            "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (59ms)\n",
            "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
            "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (34ms)\n",
            "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
            "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (33ms)\n",
            "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
            "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (32ms)\n",
            "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
            "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (36ms)\n",
            "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
            "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (30ms)\n",
            "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
            "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (39ms)\n",
            "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
            "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (39ms)\n",
            "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
            "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (34ms)\n",
            ":: resolution report :: resolve 9750ms :: artifacts dl 5310ms\n",
            "\t:: modules in use:\n",
            "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
            "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
            "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
            "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
            "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
            "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
            "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
            "\tcom.github.luben#zstd-jni;1.5.2-1 from central in [default]\n",
            "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
            "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
            "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
            "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
            "\tcom.typesafe#config;1.4.1 from central in [default]\n",
            "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
            "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
            "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
            "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;3.3.1 from central in [default]\n",
            "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
            "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
            "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
            "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
            "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
            "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
            "\t:: evicted modules:\n",
            "\torg.apache.kafka#kafka-clients;2.8.1 by [org.apache.kafka#kafka-clients;3.3.1] in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.32 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
            "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   33  |   31  |   31  |   4   ||   29  |   29  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-af4bdf18-6e56-4d59-931f-59606815db84\n",
            "\tconfs: [default]\n",
            "\t29 artifacts copied, 0 already retrieved (80760kB/363ms)\n",
            "23/09/14 14:39:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/09/14 14:39:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.10.12 (main, Jun 11 2023 05:26:28)\n",
            "Spark context Web UI available at http://512873ec716d:4041\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1694702381292).\n",
            "SparkSession available as 'spark'.\n",
            ">>> from pyspark.sql import SparkSession, DataFrame\n",
            ">>> from pyspark.sql import functions as F\n",
            ">>> from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
            ">>> from pyspark.ml import PipelineModel\n",
            ">>> import datetime\n",
            ">>> import shutil\n",
            ">>> schema = StructType()     .add(\"obs\", StringType())     .add(\"gpa\", StringType())     .add(\"iq\", StringType())     .add(\"gender\", StringType())     .add(\"concept\", StringType())\n",
            ">>> raw_files = spark     .readStream     .format(\"csv\")     .schema(schema)     .options(path=\"gpa_iq.csv\", header=True)     .load()\n",
            ">>> raw_orders = spark.readStream.     format(\"kafka\").     option(\"kafka.bootstrap.servers\", \"localhost:9092\").     option(\"subscribe\", \"gpa\").     option(\"startingOffsets\", \"earliest\").     option(\"maxOffsetsPerTrigger\", \"5\").     load()\n",
            ">>> parsed_gpa = raw_orders     .select(F.from_json(F.col(\"value\").cast(\"String\"), schema).alias(\"value\"), \"offset\")     .select(\"value.*\", \"offset\")\n",
            ">>> parsed_gpa = parsed_gpa.withColumn(\"gpa\", parsed_gpa[\"gpa\"].cast(DoubleType()))                        .withColumn(\"iq\", parsed_gpa[\"iq\"].cast(IntegerType()))                        .withColumn(\"obs\", parsed_gpa[\"obs\"].cast(IntegerType()))                        .withColumn(\"concept\", parsed_gpa[\"concept\"].cast(IntegerType()))\n",
            ">>> def console_output(df, freq):\n",
            "...     return df.writeStream         .format(\"console\")         .trigger(processingTime='%s seconds' % freq )         .options(truncate=True)         .option(\"checkpointLocation\", \"checkpoint_gpa\")         .start()\n",
            "... \n",
            ">>> load_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
            ">>> def rm_checkpoint():\n",
            "...   checkpoint_location = \"checkpoint_gpa\"\n",
            "...   try:\n",
            "...     shutil.rmtree(checkpoint_location)\n",
            "...     print(f\"Чекпоинт в {checkpoint_location} успешно удален.\")\n",
            "...   except FileNotFoundError:\n",
            "...     print(f\"Чекпоинт в {checkpoint_location} не найден.\")\n",
            "... \n",
            ">>> timed_files = raw_files.withColumn(\"p_date\", F.lit(\"load_time\"))\n",
            ">>> model_path = \"gpa_lr_Model\"\n",
            ">>> model = PipelineModel.load(model_path)\n",
            ">>> model\n",
            "PipelineModel_7091fede00dc\n",
            ">>> predictions = model.transform(parsed_gpa)\n",
            ">>> predictions = predictions.withColumn(\"date\", F.current_timestamp())\n",
            ">>> predictions.printSchema()\n",
            "root\n",
            " |-- obs: integer (nullable = true)\n",
            " |-- gpa: double (nullable = true)\n",
            " |-- iq: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- concept: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- label: double (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- rawPrediction: vector (nullable = true)\n",
            " |-- probability: vector (nullable = true)\n",
            " |-- prediction: double (nullable = false)\n",
            " |-- date: timestamp (nullable = false)\n",
            "\n",
            ">>> df = predictions.select('offset', 'gender', 'label', 'prediction', 'probability', 'date')\n",
            ">>> st = console_output(df, 15)\n",
            "23/09/14 14:45:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/14 14:45:06 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+------+------+-----+----------+--------------------+--------------------+\n",
            "|offset|gender|label|prediction|         probability|                date|\n",
            "+------+------+-----+----------+--------------------+--------------------+\n",
            "|     0|     2|  0.0|       0.0|[0.78552951318411...|2023-09-14 14:45:...|\n",
            "|     1|     2|  0.0|       1.0|[0.24687394800529...|2023-09-14 14:45:...|\n",
            "|     2|     2|  0.0|       0.0|[0.94180735756491...|2023-09-14 14:45:...|\n",
            "|     3|     2|  0.0|       0.0|[0.78806773202637...|2023-09-14 14:45:...|\n",
            "|     4|     1|  1.0|       0.0|[0.50451333473423...|2023-09-14 14:45:...|\n",
            "+------+------+-----+----------+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+------+------+-----+----------+--------------------+--------------------+\n",
            "|offset|gender|label|prediction|         probability|                date|\n",
            "+------+------+-----+----------+--------------------+--------------------+\n",
            "|     5|     2|  0.0|       0.0|[0.76000241097098...|2023-09-14 14:45:...|\n",
            "|     6|     2|  0.0|       0.0|[0.87140516365736...|2023-09-14 14:45:...|\n",
            "|     7|     2|  0.0|       0.0|[0.99272956657553...|2023-09-14 14:45:...|\n",
            "|     8|     1|  1.0|       0.0|[0.74710649370553...|2023-09-14 14:45:...|\n",
            "|     9|     2|  0.0|       1.0|[0.35680432100061...|2023-09-14 14:45:...|\n",
            "+------+------+-----+----------+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+------+------+-----+----------+--------------------+--------------------+\n",
            "|offset|gender|label|prediction|         probability|                date|\n",
            "+------+------+-----+----------+--------------------+--------------------+\n",
            "|    10|     1|  1.0|       1.0|[0.26681657766334...|2023-09-14 14:45:...|\n",
            "|    11|     1|  1.0|       0.0|[0.66243110347532...|2023-09-14 14:45:...|\n",
            "|    12|     2|  0.0|       0.0|[0.63917937825910...|2023-09-14 14:45:...|\n",
            "|    13|     1|  1.0|       0.0|[0.63050325318734...|2023-09-14 14:45:...|\n",
            "|    14|     1|  1.0|       0.0|[0.87943620740486...|2023-09-14 14:45:...|\n",
            "+------+------+-----+----------+--------------------+--------------------+\n",
            "\n",
            "st.stop()\n",
            ">>> rm_checkpoint()\n",
            "Чекпоинт в checkpoint_gpa успешно удален.\n",
            ">>> import pandas as pd\n",
            ">>> from cassandra.cluster import Cluster\n",
            ">>> cluster = Cluster(['127.0.0.1'])\n",
            ">>> session = cluster.connect()\n",
            ">>> session.execute(      \"CREATE KEYSPACE IF NOT EXISTS hw_7 \"      \"WITH REPLICATION = { \"          \"'class': 'SimpleStrategy', \"          \"'replication_factor': 1 \"      \"}\"  )\n",
            "<cassandra.cluster.ResultSet object at 0x7f00b4104f70>\n",
            ">>> session.execute(\"USE hw_7\")\n",
            "<cassandra.cluster.ResultSet object at 0x7f00b4105390>\n",
            ">>> session.execute(      'CREATE TABLE IF NOT EXISTS gpa_table ('          'offset BIGINT, '          '\"gender\" VARCHAR, '          'label DOUBLE, '          'prediction DOUBLE, '          'probability LIST<DOUBLE>, '          'date TIMESTAMP, '          'PRIMARY KEY (offset)'      ')'  )\n",
            "<cassandra.cluster.ResultSet object at 0x7f00b4105b40>\n",
            ">>> def cassandra_output_ckeckpointed(df, freq):\n",
            "...      return df.writeStream          .format(\"org.apache.spark.sql.cassandra\")          .trigger(processingTime=f'{freq} seconds')          .option(\"table\", \"gpa_table\")          .option(\"keyspace\", \"hw_7\")          .option(\"checkpointLocation\", \"checkpoint\")          .start()\n",
            "... \n",
            ">>> def convert_to_list(vector):\n",
            "...   return vector.tolist()\n",
            "... \n",
            ">>> convert_to_list_udf = F.udf(convert_to_list, ArrayType(DoubleType()))\n",
            ">>> df = df.withColumn(\"probability\", convert_to_list_udf(df[\"probability\"]))\n",
            ">>> df.printSchema()\n",
            "root\n",
            " |-- offset: long (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- label: double (nullable = false)\n",
            " |-- prediction: double (nullable = false)\n",
            " |-- probability: array (nullable = true)\n",
            " |    |-- element: double (containsNull = true)\n",
            " |-- date: timestamp (nullable = false)\n",
            "\n",
            ">>> st = cassandra_output_ckeckpointed(df, 10)\n",
            "23/09/14 14:49:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/14 14:49:04 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "23/09/14 14:49:08 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:13 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:13 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:13 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:13 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:13 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:20 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:21 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:21 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:21 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:21 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "23/09/14 14:49:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
            "st.stop()\n",
            ">>> учше()\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "NameError: name 'учше' is not defined\n",
            ">>> exit()\n"
          ]
        }
      ],
      "source": [
        "!export SPARK_KAFKA_VERSION=0.10\n",
        "!pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.kafka:kafka-clients:3.3.1,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV9USoRl-BGz"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql import SparkSession, DataFrame\n",
        "# from pyspark.sql import functions as F\n",
        "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
        "# from pyspark.ml import PipelineModel\n",
        "# import datetime\n",
        "# import shutil\n",
        "\n",
        "\n",
        "# schema = StructType()\n",
        "#     .add(\"obs\", StringType())\n",
        "#     .add(\"gpa\", StringType())\n",
        "#     .add(\"iq\", StringType())\n",
        "#     .add(\"gender\", StringType())\n",
        "#     .add(\"concept\", StringType())\n",
        "\n",
        "\n",
        "# raw_files = spark\n",
        "#     .readStream\n",
        "#     .format(\"csv\")\n",
        "#     .schema(schema)\n",
        "#     .options(path=\"gpa_iq.csv\", header=True)\n",
        "#     .load()\n",
        "\n",
        "# raw_orders = spark.readStream.\n",
        "#     format(\"kafka\").\n",
        "#     option(\"kafka.bootstrap.servers\", \"localhost:9092\").\n",
        "#     option(\"subscribe\", \"gpa\").\n",
        "#     option(\"startingOffsets\", \"earliest\").\n",
        "#     option(\"maxOffsetsPerTrigger\", \"5\").\n",
        "#     load()\n",
        "\n",
        "# parsed_gpa = raw_orders\n",
        "#     .select(F.from_json(F.col(\"value\").cast(\"String\"), schema).alias(\"value\"), \"offset\")\n",
        "#     .select(\"value.*\", \"offset\")\n",
        "\n",
        "\n",
        "# parsed_gpa = parsed_gpa.withColumn(\"gpa\", parsed_gpa[\"gpa\"].cast(DoubleType()))\n",
        "#                        .withColumn(\"iq\", parsed_gpa[\"iq\"].cast(IntegerType()))\n",
        "#                        .withColumn(\"obs\", parsed_gpa[\"obs\"].cast(IntegerType()))\n",
        "#                        .withColumn(\"concept\", parsed_gpa[\"concept\"].cast(IntegerType()))\n",
        "\n",
        "\n",
        "# def console_output(df, freq):\n",
        "#     return df.writeStream\n",
        "#         .format(\"console\")\n",
        "#         .trigger(processingTime='%s seconds' % freq )\n",
        "#         .options(truncate=True)\n",
        "#         .option(\"checkpointLocation\", \"checkpoint_gpa\")\n",
        "#         .start()\n",
        "\n",
        "\n",
        "# # проставляем время загрузки\n",
        "# load_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "\n",
        "# # функция очистки чекпоинта\n",
        "# def rm_checkpoint():\n",
        "#   checkpoint_location = \"checkpoint_gpa\"\n",
        "#   try:\n",
        "#     shutil.rmtree(checkpoint_location)\n",
        "#     print(f\"Чекпоинт в {checkpoint_location} успешно удален.\")\n",
        "#   except FileNotFoundError:\n",
        "#     print(f\"Чекпоинт в {checkpoint_location} не найден.\")\n",
        "\n",
        "\n",
        "# timed_files = raw_files.withColumn(\"p_date\", F.lit(\"load_time\"))\n",
        "\n",
        "\n",
        "# model_path = \"gpa_lr_Model\"\n",
        "# model = PipelineModel.load(model_path)\n",
        "# model\n",
        "\n",
        "# predictions = model.transform(parsed_gpa)\n",
        "# predictions = predictions.withColumn(\"date\", F.current_timestamp())\n",
        "\n",
        "# # можно посмотреть на схему, которая у нас теперь есть:\n",
        "# predictions.printSchema()\n",
        "\n",
        "# df = predictions.select('offset', 'gender', 'label', 'prediction', 'probability', 'date')\n",
        "\n",
        "# st = console_output(df, 15)\n",
        "# st.stop()\n",
        "\n",
        "# rm_checkpoint()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from cassandra.cluster import Cluster\n",
        "# cluster = Cluster(['127.0.0.1'])\n",
        "# session = cluster.connect()\n",
        "\n",
        "# session.execute(\n",
        "#      \"CREATE KEYSPACE IF NOT EXISTS hw_7 \"\n",
        "#      \"WITH REPLICATION = { \"\n",
        "#          \"'class': 'SimpleStrategy', \"\n",
        "#          \"'replication_factor': 1 \"\n",
        "#      \"}\"\n",
        "#  )\n",
        "\n",
        "\n",
        "# session.execute(\"USE hw_7\")\n",
        "\n",
        "# session.execute(\n",
        "#      'CREATE TABLE IF NOT EXISTS gpa_table ('\n",
        "#          'offset BIGINT, '\n",
        "#          '\"gender\" VARCHAR, '\n",
        "#          'label DOUBLE, '\n",
        "#          'prediction DOUBLE, '\n",
        "#          'probability LIST<DOUBLE>, '\n",
        "#          'date TIMESTAMP, '\n",
        "#          'PRIMARY KEY (offset)'\n",
        "#      ')'\n",
        "#  )\n",
        "\n",
        "\n",
        "# def cassandra_output_ckeckpointed(df, freq):\n",
        "#      return df.writeStream\n",
        "#          .format(\"org.apache.spark.sql.cassandra\")\n",
        "#          .trigger(processingTime=f'{freq} seconds')\n",
        "#          .option(\"table\", \"gpa_table\")\n",
        "#          .option(\"keyspace\", \"hw_7\")\n",
        "#          .option(\"checkpointLocation\", \"checkpoint\")\n",
        "#          .start()\n",
        "\n",
        "\n",
        "\n",
        "# def convert_to_list(vector):\n",
        "#   return vector.tolist()\n",
        "\n",
        "# convert_to_list_udf = F.udf(convert_to_list, ArrayType(DoubleType()))\n",
        "\n",
        "# df = df.withColumn(\"probability\", convert_to_list_udf(df[\"probability\"]))\n",
        "\n",
        "# df.printSchema()\n",
        "\n",
        "\n",
        "# st = cassandra_output_ckeckpointed(df, 10) !!!!ошибка\n",
        "\n",
        "# st.stop()\n",
        "\n",
        "# session.shutdown()\n",
        "# cluster.shutdown()\n",
        "# exit()"
      ],
      "metadata": {
        "id": "nDw2M98CAIgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apache-cassandra-4.0.11/bin/cqlsh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kkdfTQcLZaC",
        "outputId": "1261dbf8-bb9b-42fe-d03d-3841c863b1ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to \u001b[0;1;34mTest Cluster\u001b[0m at 127.0.0.1:9042\n",
            "[cqlsh 6.0.0 | Cassandra 4.0.11 | CQL spec 3.4.5 | Native protocol v5]\n",
            "Use HELP for help.\n",
            "cqlsh> USE hw_7;\n",
            "cqlsh:hw_7> SELECT COUNT(*) FROM gpa_table ALLOW FILTERING;\n",
            "\n",
            " \u001b[0;1;35mcount\u001b[0m\n",
            "-------\n",
            "    \u001b[0;1;32m20\u001b[0m\n",
            "\n",
            "(1 rows)\n",
            "\n",
            "Warnings :\n",
            "Aggregation query used without partition key\n",
            "\n",
            "cqlsh:hw_7> SELECT * FROM gpa_table LIMIT 15;\n",
            "\n",
            " \u001b[0;1;31moffset\u001b[0m | \u001b[0;1;35mdate\u001b[0m                            | \u001b[0;1;35mgender\u001b[0m | \u001b[0;1;35mlabel\u001b[0m | \u001b[0;1;35mprediction\u001b[0m | \u001b[0;1;35mprobability\u001b[0m\n",
            "--------+---------------------------------+--------+-------+------------+----------------------\n",
            "     \u001b[0;1;32m19\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:30.016000+0000\u001b[0m |      \u001b[0;1;33m1\u001b[0m |     \u001b[0;1;32m1\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.928038\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.071962\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "      \u001b[0;1;32m2\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:04.999000+0000\u001b[0m |      \u001b[0;1;33m2\u001b[0m |     \u001b[0;1;32m0\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.941807\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.058193\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "      \u001b[0;1;32m3\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:04.999000+0000\u001b[0m |      \u001b[0;1;33m2\u001b[0m |     \u001b[0;1;32m0\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.788068\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.211932\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "     \u001b[0;1;32m16\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:30.016000+0000\u001b[0m |      \u001b[0;1;33m1\u001b[0m |     \u001b[0;1;32m1\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.746305\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.253695\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "     \u001b[0;1;32m12\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:20.016000+0000\u001b[0m |      \u001b[0;1;33m2\u001b[0m |     \u001b[0;1;32m0\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.639179\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.360821\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "     \u001b[0;1;32m13\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:20.016000+0000\u001b[0m |      \u001b[0;1;33m1\u001b[0m |     \u001b[0;1;32m1\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.630503\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.369497\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "      \u001b[0;1;32m7\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:13.191000+0000\u001b[0m |      \u001b[0;1;33m2\u001b[0m |     \u001b[0;1;32m0\u001b[0m |          \u001b[0;1;32m0\u001b[0m |   \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.99273\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.00727\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "     \u001b[0;1;32m15\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:30.016000+0000\u001b[0m |      \u001b[0;1;33m1\u001b[0m |     \u001b[0;1;32m1\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.830734\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.169266\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "      \u001b[0;1;32m9\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:13.191000+0000\u001b[0m |      \u001b[0;1;33m2\u001b[0m |     \u001b[0;1;32m0\u001b[0m |          \u001b[0;1;32m1\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.356804\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.643196\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "      \u001b[0;1;32m4\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:04.999000+0000\u001b[0m |      \u001b[0;1;33m1\u001b[0m |     \u001b[0;1;32m1\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.504513\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.495487\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "     \u001b[0;1;32m10\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:20.016000+0000\u001b[0m |      \u001b[0;1;33m1\u001b[0m |     \u001b[0;1;32m1\u001b[0m |          \u001b[0;1;32m1\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.266817\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.733183\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "      \u001b[0;1;32m0\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:04.999000+0000\u001b[0m |      \u001b[0;1;33m2\u001b[0m |     \u001b[0;1;32m0\u001b[0m |          \u001b[0;1;32m0\u001b[0m |   \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.78553\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.21447\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "     \u001b[0;1;32m11\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:20.016000+0000\u001b[0m |      \u001b[0;1;33m1\u001b[0m |     \u001b[0;1;32m1\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.662431\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.337569\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "     \u001b[0;1;32m14\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:20.016000+0000\u001b[0m |      \u001b[0;1;33m1\u001b[0m |     \u001b[0;1;32m1\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.879436\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.120564\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "      \u001b[0;1;32m8\u001b[0m | \u001b[0;1;32m2023-09-14 14:49:13.191000+0000\u001b[0m |      \u001b[0;1;33m1\u001b[0m |     \u001b[0;1;32m1\u001b[0m |          \u001b[0;1;32m0\u001b[0m | \u001b[0;1;34m[\u001b[0m\u001b[0;1;32m0.747106\u001b[0m\u001b[0;1;34m, \u001b[0m\u001b[0;1;32m0.252894\u001b[0m\u001b[0;1;34m]\u001b[0m\n",
            "\n",
            "(15 rows)\n",
            "cqlsh:hw_7> exit;\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}