{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download joebeachcapital/gpa-and-iq\n",
        "! unzip /content/gpa-and-iq"
      ],
      "metadata": {
        "id": "d8nmnRnKc3PV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa4c7fd-7198-46b8-d011-78db769c3d28"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Downloading gpa-and-iq.zip to /content\n",
            "  0% 0.00/795 [00:00<?, ?B/s]\n",
            "100% 795/795 [00:00<00:00, 2.30MB/s]\n",
            "Archive:  /content/gpa-and-iq.zip\n",
            "  inflating: gpa_iq.csv              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mlWWfb77b00N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db202a21-b4ed-4265-b6a9-8536aa49aef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=a4d7201c3d5823af49e29443ed589ec8d336549bb188a4f135fe25819a328622\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -sSOL https://dlcdn.apache.org/kafka/3.5.0/kafka_2.13-3.5.0.tgz\n",
        "\n",
        "!tar -xzf kafka_2.13-3.5.0.tgz"
      ],
      "metadata": {
        "id": "2ubHeCsYdbO4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.5.0/config/zookeeper.properties\n",
        "!./kafka_2.13-3.5.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.5.0/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jibaCHyydf0_",
        "outputId": "09e59f91-298d-483d-ec2b-1ddbf874e5eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic gpa\n",
        "!./kafka_2.13-3.5.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic gpa_sink\n",
        "!./kafka_2.13-3.5.0/bin/kafka-topics.sh  --list --bootstrap-server localhost:9092"
      ],
      "metadata": {
        "id": "NPLVluSXdkx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ff855d-c17a-460a-cf50-7f4b0e7189ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic gpa.\n",
            "WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\n",
            "Created topic gpa_sink.\n",
            "gpa\n",
            "gpa_sink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "csvfile = open('gpa_iq.csv', 'r')\n",
        "jsonfile = open('gpa_iq.json', 'w')\n",
        "\n",
        "\n",
        "reader = csv.DictReader(csvfile)\n",
        "for row in reader:\n",
        "   json.dump(row, jsonfile)\n",
        "   jsonfile.write('\\n')\n",
        "\n",
        "jsonfile = open('gpa_iq.json', 'w')"
      ],
      "metadata": {
        "id": "HxgZfxLVdwac"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic gpa < gpa_iq.json\n",
        "!./kafka_2.13-3.5.0/bin/kafka-console-consumer.sh --topic gpa  --bootstrap-server localhost:9092 --from-beginning  --max-messages 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HadjiNmnd23D",
        "outputId": "95dc23b3-c1bb-49bb-e8b5-b14b859790de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"obs\": \"1\", \"gpa\": \"7.94\", \"iq\": \"111\", \"gender\": \"2\", \"concept\": \"67\"}\n",
            "{\"obs\": \"2\", \"gpa\": \"8.292\", \"iq\": \"107\", \"gender\": \"2\", \"concept\": \"43\"}\n",
            "{\"obs\": \"3\", \"gpa\": \"4.643\", \"iq\": \"100\", \"gender\": \"2\", \"concept\": \"52\"}\n",
            "{\"obs\": \"4\", \"gpa\": \"7.47\", \"iq\": \"107\", \"gender\": \"2\", \"concept\": \"66\"}\n",
            "{\"obs\": \"5\", \"gpa\": \"8.882\", \"iq\": \"114\", \"gender\": \"1\", \"concept\": \"58\"}\n",
            "Processed a total of 5 messages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdfs"
      ],
      "metadata": {
        "id": "N1jkPItkiVYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10689dd1-60b7-4211-b503-dd62c71b4ce5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hdfs\n",
            "  Downloading hdfs-2.7.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41.0/43.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m872.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from hdfs)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (2.31.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2023.7.22)\n",
            "Building wheels for collected packages: hdfs, docopt\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.2-py3-none-any.whl size=34168 sha256=b9359f9a35cfe9dc335364a35408967d89a09fa3c04f034a45a0cff4a11102f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/39/8e/e1905de9af8ae74911cd3e53e721995cd230816f63776e5825\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=88a2d3eff7f2dccee321848137a5ab17a4ec6674c3176071a3ffcc9f26581354\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built hdfs docopt\n",
            "Installing collected packages: docopt, hdfs\n",
            "Successfully installed docopt-0.6.2 hdfs-2.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export SPARK_KAFKA_VERSION=0.10\n",
        "!pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.kafka:kafka-clients:3.3.1"
      ],
      "metadata": {
        "id": "y7yu31oNd9oK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b745cc31-a965-4b02-b096-5c5315c2a9d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
            "org.apache.kafka#kafka-clients added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-790e51aa-5b39-4f77-ac8d-2b6d65f0c0a5;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
            "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
            "\tfound commons-logging#commons-logging;1.1.3 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
            "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
            "\tfound org.apache.kafka#kafka-clients;3.3.1 in central\n",
            "\tfound com.github.luben#zstd-jni;1.5.2-1 in central\n",
            "\tfound org.lz4#lz4-java;1.8.0 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
            ":: resolution report :: resolve 1234ms :: artifacts dl 53ms\n",
            "\t:: modules in use:\n",
            "\tcom.github.luben#zstd-jni;1.5.2-1 from central in [default]\n",
            "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
            "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
            "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;3.3.1 from central in [default]\n",
            "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
            "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
            "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
            "\t:: evicted modules:\n",
            "\torg.apache.kafka#kafka-clients;2.8.1 by [org.apache.kafka#kafka-clients;3.3.1] in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.32 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   15  |   0   |   0   |   2   ||   13  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-790e51aa-5b39-4f77-ac8d-2b6d65f0c0a5\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 13 already retrieved (0kB/15ms)\n",
            "23/09/04 17:43:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.4.1\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.10.12 (main, Jun 11 2023 05:26:28)\n",
            "Spark context Web UI available at http://595f30a86314:4040\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1693849416691).\n",
            "SparkSession available as 'spark'.\n",
            ">>> from pyspark.sql import functions as F\n",
            ">>> from pyspark.sql.types import StructType, StringType\n",
            ">>> kafka_brokers = \"localhost:9092\"\n",
            ">>> raw_orders = spark.readStream.     format(\"kafka\").     option(\"kafka.bootstrap.servers\", kafka_brokers).     option(\"subscribe\", \"gpa\").     option(\"startingOffsets\", \"earliest\").     option(\"maxOffsetsPerTrigger\", \"5\").     load()\n",
            ">>> schema = StructType()     .add(\"obs\", StringType())     .add(\"gpa\", StringType())     .add(\"iq\", StringType())     .add(\"gender\", StringType())     .add(\"concept\", StringType())\n",
            ">>> parsed_gpa = raw_orders     .select(F.from_json(F.col(\"value\").cast(\"String\"), schema).alias(\"value\"), \"offset\")     .select(\"value.*\", \"offset\")\n",
            ">>> spark.sql(\"show tables\").show()\n",
            "23/09/04 17:45:46 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
            "23/09/04 17:45:46 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
            "23/09/04 17:45:53 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
            "23/09/04 17:45:53 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.28.0.12\n",
            "23/09/04 17:45:53 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
            "23/09/04 17:45:54 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "+---------+---------+-----------+\n",
            "\n",
            ">>> def memory_sink(df, freq):\n",
            "...     return df.writeStream.format(\"memory\")\n",
            "...     .queryName(\"gpa_sink_table\")\n",
            "...     .trigger(processingTime='%s seconds' % freq )\n",
            "...     .start()\n",
            "...\n",
            ">>> stream = memory_sink(parsed_gpa, 5)\n",
            "23/09/04 17:46:30 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-aec04c84-8013-4919-ac16-11805e9850cb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "23/09/04 17:46:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/04 17:46:32 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "23/09/04 17:46:36 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 5371 milliseconds\n",
            "spark.sql(\"select * from gpa_sink_table\").show()\n",
            "+---+-----+---+------+-------+------+\n",
            "|obs|  gpa| iq|gender|concept|offset|\n",
            "+---+-----+---+------+-------+------+\n",
            "|  1| 7.94|111|     2|     67|     0|\n",
            "|  2|8.292|107|     2|     43|     1|\n",
            "|  3|4.643|100|     2|     52|     2|\n",
            "|  4| 7.47|107|     2|     66|     3|\n",
            "|  5|8.882|114|     1|     58|     4|\n",
            "|  6|7.585|115|     2|     51|     5|\n",
            "|  7| 7.65|111|     2|     71|     6|\n",
            "|  8|2.412| 97|     2|     51|     7|\n",
            "|  9|    6|100|     1|     49|     8|\n",
            "| 10|8.833|112|     2|     51|     9|\n",
            "| 11| 7.47|104|     1|     35|    10|\n",
            "| 12|5.528| 89|     1|     54|    11|\n",
            "| 13|7.167|104|     2|     54|    12|\n",
            "| 14|7.571|102|     1|     64|    13|\n",
            "| 15|  4.7| 91|     1|     56|    14|\n",
            "| 16|8.167|114|     1|     69|    15|\n",
            "| 17|7.822|114|     1|     55|    16|\n",
            "| 18|7.598|103|     1|     65|    17|\n",
            "| 19|    4|106|     2|     40|    18|\n",
            "| 20|6.231|105|     1|     66|    19|\n",
            "+---+-----+---+------+-------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            ">>> spark.sql(\"select count(*) from gpa_sink_table\").show()\n",
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|      50|\n",
            "+--------+\n",
            "\n",
            ">>> stream.stop()\n",
            ">>> def file_sink(df, freq):\n"
            "...     return df.writeStream.format(\"parquet\")\n",        
            "...     .trigger(processingTime='%s seconds' % freq )\n", 
            "...     .option(\"path\",\"my_parquet_sink\")\n", 
            "...     .option(\"checkpointLocation\", \"checkpoint_1\")\n", 
            "...     .start()\n",
            "... \n",
            ">>> stream = file_sink(parsed_gpa, 5)\n",
            "23/09/04 17:48:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/04 17:48:20 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "stream.stop()\n",
            ">>> extended_gpa = parsed_gpa.withColumn(\"my_current_time\", F.current_timestamp())\n",
            ">>> def foreach_batch_sink(df, freq):\n",
            "...     return  df\n",         
            "...     .writeStream\n",        
            "...     .foreachBatch(foreach_batch_function)\n",  
            "...     .trigger(processingTime='%s seconds' % freq )\n", 
            "...     .start()\n",
            "... \n",
            ">>> def foreach_batch_function(df, epoch_id):\n",
            "...     print(\"starting epoch \" + str(epoch_id) )\n",
            "...     print(\"average values for batch:\")\n",
            "...     df.groupBy(\"gender\").count().show()\n",
            "...     print(\"finishing epoch \" + str(epoch_id))\n",
            "... \n",
            ">>> stream = foreach_batch_sink(extended_gpa, 5)\n",
            "23/09/04 17:51:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2ff549c3-0a09-4937-9045-d5daf1aa40df. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "23/09/04 17:51:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            ">>> 23/09/04 17:51:49 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
            "starting epoch 0\n",
            "average values for batch:\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     2|    4|\n",
            "|     1|    1|\n",
            "+------+-----+\n",
            "\n",
            "finishing epoch 0\n",
            "starting epoch 1\n",
            "average values for batch:\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     2|    4|\n",
            "|     1|    1|\n",
            "+------+-----+\n",
            "\n",
            "finishing epoch 1\n",
            "starting epoch 2\n",
            "average values for batch:\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     1|    4|\n",
            "|     2|    1|\n",
            "+------+-----+\n",
            "\n",
            "finishing epoch 2\n",
            "starting epoch 3\n",
            "average values for batch:\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     1|    4|\n",
            "|     2|    1|\n",
            "+------+-----+\n",
            "\n",
            "finishing epoch 3\n",
            "starting epoch 4\n",
            "average values for batch:\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     2|    3|\n",
            "|     1|    2|\n",
            "+------+-----+\n",
            "\n",
            "finishing epoch 4\n",
            "starting epoch 5\n",
            "average values for batch:\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     2|    4|\n",
            "|     1|    1|\n",
            "+------+-----+\n",
            "\n",
            "finishing epoch 5\n",
            "starting epoch 6\n",
            "average values for batch:\n",
            "+------+-----+\n",
            "|gender|count|\n",
            "+------+-----+\n",
            "|     1|    3|\n",
            "|     2|    2|\n",
            "+------+-----+\n",
            "\n",
            "finishing epoch 6\n",
            "stream.stop()\n",
            ">>> exit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MEMORY SINK"
      ],
      "metadata": {
        "id": "TLx5kMe3jKSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType, StringType\n",
        "\n",
        "\n",
        "kafka_brokers = \"localhost:9092\"\n",
        "\n",
        "raw_orders = spark.readStream.\\\n",
        "    format(\"kafka\").\\\n",
        "    option(\"kafka.bootstrap.servers\", kafka_brokers).\\\n",
        "    option(\"subscribe\", \"gpa\").\\\n",
        "    option(\"startingOffsets\", \"earliest\").\\\n",
        "    option(\"maxOffsetsPerTrigger\", \"5\").\\\n",
        "    load()\n",
        "\n",
        "schema = StructType()\\\n",
        "    .add(\"obs\", StringType())\\\n",
        "    .add(\"gpa\", StringType())\\\n",
        "    .add(\"iq\", StringType())\\\n",
        "    .add(\"gender\", StringType())\\\n",
        "    .add(\"concept\", StringType())\n",
        "\n",
        "parsed_gpa = raw_orders\\\n",
        "    .select(F.from_json(F.col(\"value\").cast(\"String\"), schema).alias(\"value\"), \"offset\")\\\n",
        "    .select(\"value.*\", \"offset\")"
      ],
      "metadata": {
        "id": "w33chz8OeBbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"show tables\").show()"
      ],
      "metadata": {
        "id": "0yYOXfSUf6Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def memory_sink(df, freq):\n",
        "    return df.writeStream.format(\"memory\")\\\n",
        "        .queryName(\"gpa_sink_table\")\\\n",
        "        .trigger(processingTime='%s seconds' % freq )\\\n",
        "        .start()\n",
        "\n",
        "stream = memory_sink(parsed_gpa, 5)\n",
        "\n",
        "spark.sql(\"select * from gpa_sink_table\").show()\n",
        "\n",
        "spark.sql(\"select count(*) from gpa_sink_table\").show()\n",
        "\n",
        "stream.stop()"
      ],
      "metadata": {
        "id": "SwDQebFAgzcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FILE SINK"
      ],
      "metadata": {
        "id": "hmrc6wgDj3un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def file_sink(df, freq):\n",
        "    return df.writeStream.format(\"parquet\")\\\n",
        "        .trigger(processingTime='%s seconds' % freq )\\\n",
        "        .option(\"path\",\"my_parquet_sink\")\\\n",
        "        .option(\"checkpointLocation\", \"checkpoint_1\")\\\n",
        "        .start()\n",
        "\n",
        "\n",
        "stream = file_sink(parsed_gpa, 5)\n",
        "stream.stop()"
      ],
      "metadata": {
        "id": "f0DhJhxuh8vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beTCGyWysrie",
        "outputId": "5cf15e0a-6f53-4c3e-f82f-8af3ff814827"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint_1\tgpa_iq.csv\t  kafka_2.13-3.5.0.tgz\tmy_parquet_sink\n",
            "derby.log\tgpa_iq.json\t  kaggle.json\t\tsample_data\n",
            "gpa-and-iq.zip\tkafka_2.13-3.5.0  metastore_db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls my_parquet_sink"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzYE6BCK9o29",
        "outputId": "0583541c-0f7f-423a-891b-3f7473d31528"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-1a907bbc-6e0d-4f58-a0ce-039b9fc054a4-c000.snappy.parquet\n",
            "part-00000-3ea17ccc-59c8-4cfd-a2c0-acbbfaf98fc5-c000.snappy.parquet\n",
            "part-00000-57d98f53-b030-40ba-b270-ea89c2e7437e-c000.snappy.parquet\n",
            "part-00000-7c7613de-f5cd-45a5-9181-83bff6732be0-c000.snappy.parquet\n",
            "part-00000-f381b589-da48-4fc7-ac11-db699ae15d6f-c000.snappy.parquet\n",
            "_spark_metadata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_parquet(\"my_parquet_sink\")"
      ],
      "metadata": {
        "id": "FS1sXeP6JJgy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by=\"offset\").head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "rYIlAr49KIL6",
        "outputId": "7f648870-8c7f-48a2-adb8-9621ddb4e858"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   obs    gpa   iq gender concept  offset\n",
              "5    1   7.94  111      2      67       0\n",
              "6    2  8.292  107      2      43       1\n",
              "7    3  4.643  100      2      52       2\n",
              "8    4   7.47  107      2      66       3\n",
              "9    5  8.882  114      1      58       4\n",
              "15   6  7.585  115      2      51       5\n",
              "16   7   7.65  111      2      71       6\n",
              "17   8  2.412   97      2      51       7\n",
              "18   9      6  100      1      49       8\n",
              "19  10  8.833  112      2      51       9"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e1d2263-dd2d-4909-8fec-9a0d3c3b6ea3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>obs</th>\n",
              "      <th>gpa</th>\n",
              "      <th>iq</th>\n",
              "      <th>gender</th>\n",
              "      <th>concept</th>\n",
              "      <th>offset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>7.94</td>\n",
              "      <td>111</td>\n",
              "      <td>2</td>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>8.292</td>\n",
              "      <td>107</td>\n",
              "      <td>2</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>4.643</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>52</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4</td>\n",
              "      <td>7.47</td>\n",
              "      <td>107</td>\n",
              "      <td>2</td>\n",
              "      <td>66</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>8.882</td>\n",
              "      <td>114</td>\n",
              "      <td>1</td>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>7.585</td>\n",
              "      <td>115</td>\n",
              "      <td>2</td>\n",
              "      <td>51</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>7</td>\n",
              "      <td>7.65</td>\n",
              "      <td>111</td>\n",
              "      <td>2</td>\n",
              "      <td>71</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>8</td>\n",
              "      <td>2.412</td>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>51</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>49</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>10</td>\n",
              "      <td>8.833</td>\n",
              "      <td>112</td>\n",
              "      <td>2</td>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e1d2263-dd2d-4909-8fec-9a0d3c3b6ea3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3e1d2263-dd2d-4909-8fec-9a0d3c3b6ea3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3e1d2263-dd2d-4909-8fec-9a0d3c3b6ea3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8732a50c-2bb9-4a13-8c73-84a0e156de8c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8732a50c-2bb9-4a13-8c73-84a0e156de8c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8732a50c-2bb9-4a13-8c73-84a0e156de8c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KAFKA SINK"
      ],
      "metadata": {
        "id": "6YHzrP6QkjxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kafka_sink_json(df, freq):\n",
        "    return df.selectExpr(\"CAST(null AS STRING) as key\", \"CAST(to_json(struct(*)) AS STRING) as value\")\n",
        "        .writeStream\n",
        "        .format(\"kafka\")\n",
        "        .trigger(processingTime='%s seconds' % freq )\n",
        "        .option(\"topic\", \"gpa_sink\")\n",
        "        .option(\"kafka.bootstrap.servers\", kafka_brokers)\n",
        "        .option(\"checkpointLocation\", \"checkpoint_2\")\n",
        "        .start()\n",
        "\n",
        "stream = kafka_sink_json(parsed_gpa, 10)\n",
        "stream.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "YpSOgpchmslW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kafka_sink(df, freq):\n",
        "    return df.selectExpr(\"CAST(null AS STRING) as key\", \"CAST(struct(*) AS STRING) as value\")\n",
        "        .writeStream\n",
        "        .format(\"kafka\")\n",
        "        .trigger(processingTime='%s seconds' % freq )\n",
        "        .option(\"topic\", \"gpa_sink\")\n",
        "        .option(\"kafka.bootstrap.servers\", kafka_brokers)\n",
        "        .option(\"checkpointLocation\", \"checkpoint_2\")\n",
        "        .start()\n",
        "\n",
        "stream = kafka_sink(parsed_gpa, 5)\n",
        "stream.stop()"
      ],
      "metadata": {
        "id": "jcryFjM3FDF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CH BATCH SINK"
      ],
      "metadata": {
        "id": "Jdi7OuGIna9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extended_gpa = parsed_gpa.withColumn(\"my_current_time\", F.current_timestamp())\n",
        "\n",
        "def foreach_batch_sink(df, freq):\n",
        "    return  df\\\n",
        "        .writeStream\\\n",
        "        .foreachBatch(foreach_batch_function)\\\n",
        "        .trigger(processingTime='%s seconds' % freq )\\\n",
        "        .start()\n",
        "\n",
        "\n",
        "def foreach_batch_function(df, epoch_id):\n",
        "    print(\"starting epoch \" + str(epoch_id) )\n",
        "    print(\"average values for batch:\")\n",
        "    df.groupBy(\"gender\").count().show()\n",
        "    print(\"finishing epoch \" + str(epoch_id))\n",
        "\n",
        "\n",
        "stream = foreach_batch_sink(extended_gpa, 5)\n",
        "stream.stop()"
      ],
      "metadata": {
        "id": "bI7FDWZhncBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}